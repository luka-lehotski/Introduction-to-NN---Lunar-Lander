# -*- coding: utf-8 -*-
"""Lunar_Lander_EN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e2iQMrq42VdjpaI5Ff-enPJ0j8jSbwGp

## **Technical Preparation**


---
"""

# biblioteke za snimanje video zapisa
!apt-get install -y xvfb x11-utils
!pip install pyvirtualdisplay==0.2.*

!pip install gym==0.23.1
!pip install box2d-py
!pip install pygame
!pip install stable_baselines3
!pip install gym==0.23.1

# ovim se povezujemo sa Google Drive-om i podacima u njemu
from google.colab import drive
drive.mount('/content/drive')

# importovanje potrebnih biblioteka i priprema metode za prikaz snimljenog videa.
import gym
from base64 import b64encode
import numpy as np
from IPython.display import HTML
from gym.wrappers.monitoring.video_recorder import VideoRecorder
from pyvirtualdisplay import Display

# importovanje ostalih biblioteka koje koristimo
import keras
from keras import models
from keras import layers

def render_mp4(videopath: str) -> str:
  """
  Gets a string containing a b4-encoded version of the MP4 video
  at the specified path.
  """
  mp4 = open(videopath, 'rb').read()
  base64_encoded_mp4 = b64encode(mp4).decode()
  return f'<video width=400 controls><source src="data:video/mp4;' \
         f'base64,{base64_encoded_mp4}" type="video/mp4"></video>'


display = Display(visible=False, size=(1400, 900))
_ = display.start()

import numpy as np
np.set_printoptions(precision=3)

import matplotlib.pyplot as plt

from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.evaluation import evaluate_policy

projekat_folder = "drive/My Drive/UMU/projekat/"
video_putanja = projekat_folder+"video.mp4"

"""## **Environment - LunarLander-v2**"""

import gym

# učitavamo LunarLander okruženje
ime_okruzenja = "LunarLander-v2"
okruzenje = gym.make(ime_okruzenja)
spec = gym.spec(ime_okruzenja) # specifikacije okruženja
seed = 42 # fiksiranje nasumicnosti da bi se okruzenje isto ponasalo kad ponovimo program
okruzenje.reset(seed=seed)

print(f"Akcije okruženja: { okruzenje.action_space }")
print(f"Dimenzije stanja: { okruzenje.observation_space }")
print(f"Maksimalan broj koraka: { spec.max_episode_steps }")
print(f"Opseg vrednosti nagrade: { okruzenje.reward_range }")
print(f"Prag nagrade: { spec.reward_threshold }")

"""# **Random**

Here we will see how successfully (or unsuccessfully) the agent would solve the problem by randomly selecting actions - without any learning algorithms.
"""

video = VideoRecorder(okruzenje, video_putanja)

korak=0
ukupna_nagrada = 0
gotovo=False

stanje = okruzenje.reset()
while not gotovo:
  akcija = okruzenje.action_space.sample() # za početak, u svakom koraku izabiremo akciju random, iz skupa mogućih akcija u okruženju.
  stanje, nagrada, gotovo, info = okruzenje.step(akcija) # prosleđujemo odabranu akciju, okruženje vraća novo stanje, novu nagradu, informaciju da li je epizoda završila
  ukupna_nagrada += nagrada
  korak+=1

  print(f"Step: {korak},\t izabrana akcija: {akcija},\t novo stanje: {stanje},\t nagrada: {nagrada},\t gotova epizoda {gotovo},\t  info {info}")

  video.capture_frame()
video.close()
print(f"Ukupna nagrada: { ukupna_nagrada }")

HTML(render_mp4(video_putanja))

"""# **A2C**

This algorithm works with both continuous and discrete values. It optimizes the policy and is an improvement over Reinforce, as it learns the state value function V(s) simultaneously
"""

from stable_baselines3 import A2C
import torch

ime_okruzenja = "LunarLander-v2"
okruzenje = gym.make(ime_okruzenja)
spec = gym.spec(ime_okruzenja)
okruzenje.seed(42)

"""**Initial analysis with the algorithm's default parameters**

By calling the *model.learn* function multiple times and tracking the best model during training, we can determine the optimal number of steps. We always call the function for 500k steps (the more steps, the better, just in case).
"""

paralelno_okruzenje=make_vec_env("LunarLander-v2",n_envs=4,seed=42)

najbolji_model_eksperiment1_putanja = projekat_folder + "best_modelA2C_500k_n_envs=4"
logovi_evaluacije_eksperiment1_putanja = projekat_folder + "log_best_modelA2C_500k_n_envs=4"

eval_callback = EvalCallback(okruzenje, best_model_save_path=najbolji_model_eksperiment1_putanja,
                             log_path=logovi_evaluacije_eksperiment1_putanja, eval_freq=1000,
                             deterministic=True, render=False)

# eval_callback nam govori kada dodjemo do boljeg modela; štaviše, pamti ga

model = A2C('MlpPolicy',paralelno_okruzenje,verbose=1)

model.learn(500000, callback=eval_callback,log_interval=5000)

"""Graphical representation (optional)

The code cell above should be run multiple times, as mentioned earlier. Some parameters should not be analyzed based on just one model, as this could lead to incorrect conclusions.  

Unfortunately, we have saved only two models, but that will be sufficient for the analysis description. (Training was run 8 times, and those results support the conclusion about the optimal number of steps, which we derive a few cells below.)
"""

putanja_logova=projekat_folder+"log_best_modelA2C_500k_n_envs=4/evaluations.npz"
logovi1=np.load(putanja_logova)
print(f"Dostupni podfajlovi: \n{logovi1.files}")

putanja_logova=projekat_folder+"log_best_modelA2C_500k/evaluations.npz"
logovi2=np.load(putanja_logova)
print(f"Dostupni podfajlovi: \n{logovi2.files}")

x1 = logovi1['timesteps'][5:]
y1 = np.mean(logovi1['results'][5:], axis=1)

x2 = logovi2['timesteps'][5:]
y2 = np.mean(logovi2['results'][5:], axis=1)

# ne crtamo prvih 5 tačaka, jer su vrednosti na y - osi ogromne
# i ne vide se lepo nagrade na većem rednom broju koraka

fig, (ax1, ax2) = plt.subplots(1, 2,figsize=[13,4])

ax1.plot(x1, y1)
ax2.plot(x2, y2)

ax1.set_title('Prvi model')
ax1.set_xlabel('korak')
ax1.set_ylabel('nagrada')

ax2.set_title('Drugi model')
ax2.set_xlabel('korak')
ax2.set_ylabel('nagrada')

"""More than 400k steps are unnecessary (there is no better model after 400k). In fact, 300k steps are sufficient.  

Notice the spikes in the graphs—this is why *eval_callback* is essential.  

The graphs vary. If we want the model to be the same every time we rerun the training cell, we need to set an integer for the environment seed and use the seed parameter when declaring A2C.

**Polisa**
"""

#prvo probamo sa defaultnom polisom [64,64]

paralelno_okruzenje=make_vec_env("LunarLander-v2",n_envs=4,seed=42) #treniramo u četvorostrukom okruženju, zbog brzine

najbolji_model_eksperiment1_putanja = projekat_folder + "proba"
logovi_evaluacije_eksperiment1_putanja = projekat_folder + "log_proba"

eval_callback = EvalCallback(okruzenje, best_model_save_path=najbolji_model_eksperiment1_putanja,
                             log_path=logovi_evaluacije_eksperiment1_putanja, eval_freq=1000,
                             deterministic=True, render=False)

model = A2C('MlpPolicy',paralelno_okruzenje,seed=42,verbose=1)
model.learn(100000, callback=eval_callback,log_interval=5000)

#probamo sa manjom polisom [40,32]

policy_kwargs = dict(activation_fn=torch.nn.ReLU,
                     net_arch=[dict(pi=[40, 32], vf=[40, 32])])
paralelno_okruzenje=make_vec_env("LunarLander-v2",n_envs=4,seed=42)

najbolji_model_eksperiment1_putanja = projekat_folder + "proba_sa_policy_kwargs"
logovi_evaluacije_eksperiment1_putanja = projekat_folder + "log_proba_sa_policy_kwargs"

eval_callback = EvalCallback(okruzenje, best_model_save_path=najbolji_model_eksperiment1_putanja,
                             log_path=logovi_evaluacije_eksperiment1_putanja, eval_freq=1000,
                             deterministic=True, render=False)



model = A2C('MlpPolicy',paralelno_okruzenje,policy_kwargs=policy_kwargs,seed=42,verbose=1)

model.learn(100000, callback=eval_callback,log_interval=5000)

#probamo sa malo većom polisom, ali manjom od defaultne (pošto onaj rezultat nije obećavajuć)

policy_kwargs = dict(activation_fn=torch.nn.ReLU,
                     net_arch=[dict(pi=[50, 48], vf=[50, 48])])
paralelno_okruzenje=make_vec_env("LunarLander-v2",n_envs=4,seed=42)

najbolji_model_eksperiment1_putanja = projekat_folder + "best_modelA2C[52,48]"
logovi_evaluacije_eksperiment1_putanja = projekat_folder + "log_best_modelA2C[52,48]"

eval_callback = EvalCallback(okruzenje, best_model_save_path=najbolji_model_eksperiment1_putanja,
                             log_path=logovi_evaluacije_eksperiment1_putanja, eval_freq=1000,
                             deterministic=True, render=False)



model = A2C('MlpPolicy',paralelno_okruzenje,policy_kwargs=policy_kwargs,seed=42,verbose=1)

model.learn(100000, callback=eval_callback,log_interval=5000)

#probamo sa većom polisom [82,82]

policy_kwargs = dict(activation_fn=torch.nn.ReLU,
                     net_arch=[dict(pi=[82, 82], vf=[82, 82])])
paralelno_okruzenje=make_vec_env("LunarLander-v2",n_envs=4,seed=42)

najbolji_model_eksperiment1_putanja = projekat_folder + "best_modelA2C[82,82]'"
logovi_evaluacije_eksperiment1_putanja = projekat_folder + "log_best_modelA2C[82,82]'"

eval_callback = EvalCallback(okruzenje, best_model_save_path=najbolji_model_eksperiment1_putanja,
                             log_path=logovi_evaluacije_eksperiment1_putanja, eval_freq=1000,
                             deterministic=True, render=False)



model = A2C('MlpPolicy',paralelno_okruzenje,policy_kwargs=policy_kwargs,seed=42,verbose=1)

model.learn(100000, callback=eval_callback,log_interval=5000)

"""Evaluation

Evaluation is **essential** because if we only looked at the training results to compare which parameter is better (in this case, the policy size), the conclusions could be completely inaccurate (see the gamma section below).  

To prevent training from taking too long (there are 100k evaluations), the default number of episodes for evaluation is 5 (as a default value). Post-training evaluation is more precise and accurate since it runs on 100 episodes.
"""

ime_okruzenja = "LunarLander-v2"
okruzenje = gym.make(ime_okruzenja)
spec = gym.spec(ime_okruzenja)
okruzenje.seed(83) # fiksiranje nasumicnosti da bi se okruzenje isto ponasalo kad ponovimo program

model = A2C.load(projekat_folder + "best_modelA2C[82,82]'/best_model")

mean_reward, std_reward = evaluate_policy(model, okruzenje, n_eval_episodes=100)
print(f"Srednja nagrada: {mean_reward}, st. devijacija nagrade: {std_reward}")

"""Here, the evaluation results during training (see the output displayed during training) and after training are consistent, making the graph valid for analysis.

Graphical representation of the results

We emphasize once again: it makes no sense to draw a graph if we haven't previously evaluated the model and checked if the evaluations are consistent!
"""

#učitavamo vrednosti tokom treninga (korake, nagrade i dužine epizoda)

putanja_logova=projekat_folder+"log_proba/evaluations.npz"
logovi_1=np.load(putanja_logova)
print(f"Dostupni podfajlovi: \n{logovi_1.files}")

putanja_logova=projekat_folder+"log_proba_sa_policy_kwargs/evaluations.npz"
logovi_2=np.load(putanja_logova)

putanja_logova=projekat_folder+"log_best_modelA2C[52,48]/evaluations.npz"
logovi_3=np.load(putanja_logova)
print(f"Dostupni podfajlovi: \n{logovi_3.files}")

putanja_logova=projekat_folder+"log_best_modelA2C[82,82]'/evaluations.npz"
logovi_4=np.load(putanja_logova)
print(f"Dostupni podfajlovi: \n{logovi_4.files}")

# crtamo grafik za vizuelno poređenje defaultne i manjih polisa

n=5

x1 = logovi_1['timesteps'][n:]
y1 = np.mean(logovi_1['results'][n:], axis=1)
plt.plot(x1,y1,label="default [64,64]",color="red")

x2 = logovi_2['timesteps'][n:]
y2 = np.mean(logovi_2['results'][n:], axis=1)
plt.plot(x2,y2,label="[40,32]",color="green")

x3 = logovi_3['timesteps'][n:]
y3 = np.mean(logovi_3['results'][n:], axis=1)
plt.plot(x3,y3,label="[52,48]",color="blue")

plt.title("Poređenje defaultne i manjih polisa")
plt.xlabel('korak')
plt.ylabel('nagrada')
plt.legend(loc="lower right")

# crtamo grafik za vizuelno poređenje defaultne i veće polise

n=5

x1 = logovi_1['timesteps'][n:]
y1 = np.mean(logovi_1['results'][n:], axis=1)
plt.plot(x1,y1,label="default [64,64]",color="red")

x4 = logovi_4['timesteps'][n:]
y4 = np.mean(logovi_4['results'][n:], axis=1)
plt.plot(x4,y4,label="[82,82]",color="green")

plt.title("Poređenje defaultne i veće polise")
plt.xlabel('korak')
plt.ylabel('nagrada')
plt.legend(loc="lower right")

"""The last graph is a bit tricky. Namely, at the very end, the green policy looks slightly better. To be sure, we will test these two cases again, this time for 300k steps."""

# polisa po default-u, 300k koraka
paralelno_okruzenje=make_vec_env("LunarLander-v2",n_envs=4,seed=42)

najbolji_model_eksperiment1_putanja = projekat_folder + "proba_seed=42_300k"
logovi_evaluacije_eksperiment1_putanja = projekat_folder + "log_proba_seed=42_300k"

eval_callback = EvalCallback(okruzenje, best_model_save_path=najbolji_model_eksperiment1_putanja,
                             log_path=logovi_evaluacije_eksperiment1_putanja, eval_freq=1000,
                             deterministic=True, render=False)

model = A2C('MlpPolicy',paralelno_okruzenje,seed=42,verbose=1)
model.learn(300000, callback=eval_callback,log_interval=5000)

# polisa [82,82], 300k koraka

policy_kwargs = dict(activation_fn=torch.nn.ReLU,
                     net_arch=[dict(pi=[82, 82], vf=[82, 82])])
paralelno_okruzenje=make_vec_env("LunarLander-v2",n_envs=4,seed=42)

najbolji_model_eksperiment1_putanja = projekat_folder + "best_modelA2C[82,82]_seed=42"
logovi_evaluacije_eksperiment1_putanja = projekat_folder + "log_best_modelA2C[82,82]_seed=42"

eval_callback = EvalCallback(okruzenje, best_model_save_path=najbolji_model_eksperiment1_putanja,
                             log_path=logovi_evaluacije_eksperiment1_putanja, eval_freq=1000,
                             deterministic=True, render=False)



model = A2C('MlpPolicy',paralelno_okruzenje,policy_kwargs=policy_kwargs,seed=42,verbose=1)

model.learn(300000, callback=eval_callback,log_interval=5000)

# ponovo učitavanja

putanja_logova=projekat_folder+"log_proba_seed=42_300k/evaluations.npz"
logovi_1=np.load(putanja_logova)

putanja_logova=projekat_folder+"log_best_modelA2C[82,82]_seed=42/evaluations.npz"
logovi_4=np.load(putanja_logova)

# ponovo grafik
n=5

x1 = logovi_1['timesteps'][n:]
y1 = np.mean(logovi_1['results'][n:], axis=1)
plt.plot(x1,y1,label="default [64,64]",color="red")

x4 = logovi_4['timesteps'][n:]
y4 = np.mean(logovi_4['results'][n:], axis=1)
plt.plot(x4,y4,label="[82,82]",color="green")

plt.title("Poređenje defaultne i veće polise na 300k koraka")
plt.xlabel('korak')
plt.ylabel('nagrada')
plt.legend(loc="lower right")

"""Now the situation is clear – it's best to use the default policy.

**Gama**
"""

# najbolji model kada je gama po default-u već imamo iz prethodnog  u "proba_seed=42_300k"
# (odeljak "Polisa" --> "Grafičko prikazivanje rezultata")

# gama=0.95
paralelno_okruzenje=make_vec_env("LunarLander-v2",n_envs=4,seed=42)

najbolji_model_eksperiment1_putanja = projekat_folder + "0.95p"
logovi_evaluacije_eksperiment1_putanja = projekat_folder + "log_0.95p"

eval_callback = EvalCallback(okruzenje, best_model_save_path=najbolji_model_eksperiment1_putanja,
                             log_path=logovi_evaluacije_eksperiment1_putanja, eval_freq=1000,
                             deterministic=True, render=False)



model = A2C('MlpPolicy',paralelno_okruzenje,gamma=0.95,seed=42,verbose=1)

model.learn(300000, callback=eval_callback,log_interval=5000)

"""By evaluating these two models (cells below), we get the following results:  

- **-2 ± 162** (model with gamma = 0.99 - default)  
- **90 ± 133** (model with gamma = 0.95)  

We observe that the second case performs better. Now, let's try the model where gamma = 0.98.
"""

#gama=0.98

paralelno_okruzenje=make_vec_env("LunarLander-v2",n_envs=4,seed=42)

najbolji_model_eksperiment1_putanja = projekat_folder + "0.98p"
logovi_evaluacije_eksperiment1_putanja = projekat_folder + "log_0.98p"

eval_callback = EvalCallback(okruzenje, best_model_save_path=najbolji_model_eksperiment1_putanja,
                             log_path=logovi_evaluacije_eksperiment1_putanja, eval_freq=1000,
                             deterministic=True, render=False)



model = A2C('MlpPolicy',paralelno_okruzenje,gamma=0.98,seed=42,verbose=1)

model.learn(300000, callback=eval_callback,log_interval=5000)

"""This model is even better: **164 ± 105**.  

Now we can make a hypothesis – the further we move away from the value **0.98**, the worse the results get. Let's verify it.
"""

# gama=0.97

paralelno_okruzenje=make_vec_env("LunarLander-v2",n_envs=4,seed=42)

najbolji_model_eksperiment1_putanja = projekat_folder + "0.97"
logovi_evaluacije_eksperiment1_putanja = projekat_folder + "log_0.97"

eval_callback = EvalCallback(okruzenje, best_model_save_path=najbolji_model_eksperiment1_putanja,
                             log_path=logovi_evaluacije_eksperiment1_putanja, eval_freq=1000,
                             deterministic=True, render=False)



model = A2C('MlpPolicy',paralelno_okruzenje,gamma=0.97,seed=42,verbose=1)

model.learn(300000, callback=eval_callback,log_interval=5000)

"""However, our hypothesis is incorrect, as the results for **gamma = 0.97** are even better (**212 ± 73**).  

Now, let's assume that **0.97** is the "threshold" value for gamma, meaning that the further we move away from it, the worse the results get. Let's test this assumption.
"""

# gama=0.96

paralelno_okruzenje=make_vec_env("LunarLander-v2",n_envs=4,seed=42)

najbolji_model_eksperiment1_putanja = projekat_folder + "0.96"
logovi_evaluacije_eksperiment1_putanja = projekat_folder + "log_0.96"

eval_callback = EvalCallback(okruzenje, best_model_save_path=najbolji_model_eksperiment1_putanja,
                             log_path=logovi_evaluacije_eksperiment1_putanja, eval_freq=1000,
                             deterministic=True, render=False)



model = A2C('MlpPolicy',paralelno_okruzenje,gamma=0.96,seed=42,verbose=1)

model.learn(300000, callback=eval_callback,log_interval=5000)

"""The assumption seems to be quite accurate. By evaluating in several cells below, we get the following result for this model: **151 ± 108**.  

So, it performs worse than the model with **gamma = 0.97**.

The evaluation results show a strictly decreasing trend when starting from the best model (**gamma = 0.97**):  

- **212 ± 73 (0.97) > 151 ± 108 (0.96) > 90 ± 133 (0.95)**  
- **212 ± 73 (0.97) > 164 ± 105 (0.98) > -17 ± 173 (0.99)**

Just to be sure, and out of curiosity, let's also test **gamma = 0.995** (which is greater than the default).
"""

# gama=0.995

paralelno_okruzenje=make_vec_env("LunarLander-v2",n_envs=4,seed=42)

najbolji_model_eksperiment1_putanja = projekat_folder + "0.995"
logovi_evaluacije_eksperiment1_putanja = projekat_folder + "log_0.995"

eval_callback = EvalCallback(okruzenje, best_model_save_path=najbolji_model_eksperiment1_putanja,
                             log_path=logovi_evaluacije_eksperiment1_putanja, eval_freq=1000,
                             deterministic=True, render=False)



model = A2C('MlpPolicy',paralelno_okruzenje,gamma=0.995,seed=42,verbose=1)

model.learn(300000, callback=eval_callback,log_interval=5000)

"""We get a somewhat expected result (**last cell in "Evaluation"**): **-7 ± 157**.  

As expected, it performs worse than the model with **gamma = 0.97**, but the sequence is no longer strictly decreasing:  

**212 ± 73 (0.97) > 164 ± 105 (0.98) > -17 ± 173 (0.99) < -7 ± 157 (0.995)**

Let's draw a graph as well, to better understand the things we are talking about.
"""

x=['0.95','0.96','0.97','0.98','0.99','0.995']
y=[90.19,151.11,212.96,164.7,-17.18,-7.2]
e=[133.13,108.38,73.83,105.38,173.86,157.27]

plt.title("Poređenje parametara gama")
plt.xlabel('gama')
plt.ylabel('nagrada')

plt.errorbar(x, y, e, linestyle='None', marker='o')

"""Evaluation"""

ime_okruzenja = "LunarLander-v2"
okruzenje = gym.make(ime_okruzenja)
spec = gym.spec(ime_okruzenja)
okruzenje.seed(83)

model = A2C.load(projekat_folder + "proba_seed=42_300k/best_model")

mean_reward, std_reward = evaluate_policy(model, okruzenje, n_eval_episodes=100)
print(f"Srednja nagrada: {mean_reward}, st. devijacija nagrade: {std_reward}")

ime_okruzenja = "LunarLander-v2"
okruzenje = gym.make(ime_okruzenja)
spec = gym.spec(ime_okruzenja)
okruzenje.seed(83)

model = A2C.load(projekat_folder + "0.95p/best_model")

mean_reward, std_reward = evaluate_policy(model, okruzenje, n_eval_episodes=100)
print(f"Srednja nagrada: {mean_reward}, st. devijacija nagrade: {std_reward}")

ime_okruzenja = "LunarLander-v2"
okruzenje = gym.make(ime_okruzenja)
spec = gym.spec(ime_okruzenja)
okruzenje.seed(83)

model = A2C.load(projekat_folder + "0.98p/best_model")

mean_reward, std_reward = evaluate_policy(model, okruzenje, n_eval_episodes=100)
print(f"Srednja nagrada: {mean_reward}, st. devijacija nagrade: {std_reward}")

ime_okruzenja = "LunarLander-v2"
okruzenje = gym.make(ime_okruzenja)
spec = gym.spec(ime_okruzenja)
okruzenje.seed(83)

model = A2C.load(projekat_folder + "0.97/best_model")

mean_reward, std_reward = evaluate_policy(model, okruzenje, n_eval_episodes=100)
print(f"Srednja nagrada: {mean_reward}, st. devijacija nagrade: {std_reward}")

ime_okruzenja = "LunarLander-v2"
okruzenje = gym.make(ime_okruzenja)
spec = gym.spec(ime_okruzenja)
okruzenje.seed(83)

model = A2C.load(projekat_folder + "0.96/best_model")

mean_reward, std_reward = evaluate_policy(model, okruzenje, n_eval_episodes=100)
print(f"Srednja nagrada: {mean_reward}, st. devijacija nagrade: {std_reward}")

ime_okruzenja = "LunarLander-v2"
okruzenje = gym.make(ime_okruzenja)
spec = gym.spec(ime_okruzenja)
okruzenje.seed(83)

model = A2C.load(projekat_folder + "0.995/best_model")

mean_reward, std_reward = evaluate_policy(model, okruzenje, n_eval_episodes=100)
print(f"Srednja nagrada: {mean_reward}, st. devijacija nagrade: {std_reward}")

"""Graphical representation of results"""

#učitavamo vrednosti tokom treninga (korake, nagrade i dužine epizoda)



putanja_logova=projekat_folder+"log_0.97/evaluations.npz"
logovi_1=np.load(putanja_logova)

putanja_logova=projekat_folder+"log_0.95p/evaluations.npz"
logovi_3=np.load(putanja_logova)
print(f"Dostupni podfajlovi: \n{logovi_3.files}")

n=5

x1 = logovi_1['timesteps'][n:]
y1 = np.mean(logovi_1['results'][n:], axis=1)
plt.plot(x1,y1,label=" 0.97",color="red")

x3 = logovi_3['timesteps'][n:]
y3 = np.mean(logovi_3['results'][n:], axis=1)
plt.plot(x2,y2,label="0.95",color="blue")


plt.legend(loc="lower right")

"""Here's what we mentioned earlier (in 'Policy' --> 'Evaluation'). The graph clearly shows that the model with gamma = 0.95 is better than the one with gamma = 0.97, even though we know that's not the case.

To avoid the impression that we've only found one instance where the red model is better than the blue one, we can evaluate it without a seed.
The red model still performs better.

### **Conclusion**

The best model using A2C would be with the following parameters: 300k steps, the default policy (2 layers with 64 neurons each), and gamma=0.97.
"""

# taj model je već napravljen gore i sad samo treba da ga preuzmemo
model = A2C.load(projekat_folder + "0.97/best_model")

# u ovoj ćeliji možemo da treniramo model od početka sa tim parametrima (nema potreba ako već imamo gotov model)

paralelno_okruzenje=make_vec_env("LunarLander-v2",n_envs=4)

najbolji_model_eksperiment1_putanja = projekat_folder + "najbolji_model"
logovi_evaluacije_eksperiment1_putanja = projekat_folder + "najbolji_model"

eval_callback = EvalCallback(okruzenje, best_model_save_path=najbolji_model_eksperiment1_putanja,
                             log_path=logovi_evaluacije_eksperiment1_putanja, eval_freq=1000,
                             deterministic=True, render=False)


model = A2C('MlpPolicy',paralelno_okruzenje,gamma=0.97,seed=42,verbose=1)
model.learn(300000, callback=eval_callback,log_interval=5000)

# evaluacija - srednja nagrada i std

okruzenje.reset()

mean_reward, std_reward = evaluate_policy(model, okruzenje, n_eval_episodes=100)
print(f"Srednja nagrada: {mean_reward}, st. devijacija nagrade: {std_reward}")

# evaluacija - procenat pobede
okruzenje.reset()

episode_reward, episode_len = evaluate_policy(model, okruzenje, n_eval_episodes=100,return_episode_rewards=True)

episode_reward_np=np.array(episode_reward)
len(episode_reward_np[episode_reward_np>=200])

# da vidimo i kako to izgleda na videu :-)

video = VideoRecorder(okruzenje, video_putanja)

gotovo = False
stanje = okruzenje.reset()  # resetujemo okruženje, započinjemo novu epizodu i dobijamo referencu na početno stanje
ukupna_nagrada = 0
while not gotovo:
    akcija, _ = model.predict(stanje)
    stanje, nagrada, gotovo, info = okruzenje.step(akcija)
    ukupna_nagrada += nagrada
    video.capture_frame()
video.close()
print(f"Ukupna nagrada {ukupna_nagrada} ")

HTML(render_mp4(video_putanja))

"""# **PPO - Proximal Policy Optimization**
---


"""

from stable_baselines3 import PPO

"""**PPO** essentially represents an improved A2C algorithm. Its changes during training, unlike A2C, are more stable, and that's why let's see what impact this will have on the final results of the trained agent.

*Will it truly be better than A2C?*
"""

seed = 42
paralelno_okruzenje = make_vec_env(ime_okruzenja, n_envs=20)
paralelno_okruzenje.seed(seed)

eval_okruzenje = gym.make(ime_okruzenja)
eval_okruzenje.seed(seed)

"""**First, we will test the results with the parameters that performed best in the general case...**"""

najbolji_model_putanja = projekat_folder + "best_modelPPO[40,32]_100k_n_envs=10"
logovi_evaluacije_putanja = projekat_folder + "logs_best_modelPPO[40,32]_100k_n_envs=10"

eval_callback = EvalCallback(eval_okruzenje, best_model_save_path=najbolji_model_putanja,
                             log_path=logovi_evaluacije_putanja, eval_freq=1000,
                             deterministic=True, render=False)

policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=[dict(pi=[40,32], vf=[40,32])])
PPO_model = PPO("MlpPolicy", okruzenje, gamma = 0.99, policy_kwargs=policy_kwargs, verbose = 1)
PPO_model.learn(total_timesteps=100000, log_interval=1000, callback=eval_callback)
PPO_model = PPO.load(najbolji_model_putanja + "/best_model")

"""Now that we have a trained model, we can **evaluate** it and check how it handles the problem in the given environment."""

from stable_baselines3.common.evaluation import evaluate_policy

eval_okruzenje.reset(seed=seed)
mean_reward, std_reward = evaluate_policy(PPO_model, eval_okruzenje, n_eval_episodes=100)
print(f"Srednja nagrada: {mean_reward}, st. devijacija nagrade: {std_reward}")

"""The results do not exceed the reward threshold and are not satisfactory, but we can easily change this by adjusting the model parameters.

It is possible that we underestimated the complexity of the problem, so let's expand the neural network for the policy and value function. We will start with a size of [64, 64], which is the default for the PPO(...) method.
"""

najbolji_model_putanja = projekat_folder + "best_modelPPO[64,64]_100k_n_envs=10"
logovi_evaluacije_putanja = projekat_folder + "logs_best_modelPPO[64,64]_100k_n_envs=10"

eval_callback = EvalCallback(eval_okruzenje, best_model_save_path=najbolji_model_putanja,
                             log_path=logovi_evaluacije_putanja, eval_freq=1000,
                             deterministic=True, render=False)

policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=[dict(pi=[64,64], vf=[64,64])])
PPO_model = PPO("MlpPolicy", okruzenje, gamma = 0.99, policy_kwargs=policy_kwargs, verbose = 1)
PPO_model.learn(total_timesteps=100000, log_interval=1000, callback=eval_callback)
PPO_model = PPO.load(najbolji_model_putanja + "/best_model")

eval_okruzenje.reset(seed=seed)
mean_reward, std_reward = evaluate_policy(PPO_model, eval_okruzenje, n_eval_episodes=100)
print(f"Srednja nagrada: {mean_reward}, st. devijacija nagrade: {std_reward}")

"""Improvement is visible, but it can still be better."""

najbolji_model_putanja = projekat_folder + "best_modelPPO[128,128]_100k_n_envs=10"
logovi_evaluacije_putanja = projekat_folder + "logs_best_modelPPO[128,128]_100k_n_envs=10"

eval_callback = EvalCallback(eval_okruzenje, best_model_save_path=najbolji_model_putanja,
                             log_path=logovi_evaluacije_putanja, eval_freq=1000,
                             deterministic=True, render=False)

policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=[dict(pi=[128,128], vf=[128,128])])
PPO_model = PPO("MlpPolicy", okruzenje, gamma = 0.99, policy_kwargs=policy_kwargs, verbose = 1)
PPO_model.learn(total_timesteps=100000, log_interval=1000, callback=eval_callback)
PPO_model = PPO.load(najbolji_model_putanja + "/best_model")

"""The results still do not exceed the reward threshold and are not satisfactory, but why is that? The answer lies in the logs!"""

putanja_logova = logovi_evaluacije_putanja + "/evaluations.npz"
logovi_eksperimenata = np.load(putanja_logova)

import matplotlib.pyplot as plt

x = logovi_eksperimenata['timesteps'][:]
y = np.mean(logovi_eksperimenata['results'][:], axis=1)
plt.xlabel="koraci"
plt.ylabel="rezultati"
plt.plot(x,y)

print(x)
print(y)

"""Now we see, from the shape of the curve, that the poor results may stem from the small number of steps for which the agent is trained. It should, after all, become better acquainted with the environment it is in.

In the next training session, we will try a significantly larger number of steps to see what impact this has on the solution...
"""

najbolji_model_putanja = projekat_folder + "best_modelPPO[128,128]_1000k_n_envs=10"
logovi_evaluacije_putanja = projekat_folder + "logs_best_modelPPO[128,128]_1000k_n_envs=10"

eval_callback = EvalCallback(eval_okruzenje, best_model_save_path=najbolji_model_putanja,
                             log_path=logovi_evaluacije_putanja, eval_freq=1000,
                             deterministic=True, render=False)

policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=[dict(pi=[40,32], vf=[40,32])])
PPO_model = PPO("MlpPolicy", okruzenje, gamma = 0.99, policy_kwargs=policy_kwargs, verbose = 1)
PPO_model.learn(total_timesteps=1000000, log_interval=1000, callback=eval_callback)
PPO_model = PPO.load(najbolji_model_putanja + "/best_model")

eval_okruzenje.reset(seed=seed)
mean_reward, std_reward = evaluate_policy(PPO_model, eval_okruzenje, n_eval_episodes=100)
print(f"Srednja nagrada: {mean_reward}, st. devijacija nagrade: {std_reward}")

putanja_logova = logovi_evaluacije_putanja + "/evaluations.npz"
logovi_eksperimenata = np.load(putanja_logova)

x = logovi_eksperimenata['timesteps'][:]
y = np.mean(logovi_eksperimenata['results'][:], axis=1)
plt.xlabel="koraci"
plt.ylabel="rezultati"
plt.plot(x,y)

print(x)
print(y)

"""Now we see the best result! During the local work on the project, the evaluation results of this model were:

Mean reward: 275.7122282708247, reward standard deviation: 17.08913631593638

This means that the agent won **every** episode, and here we see that, at first glance, a small difference between the A2C and PPO algorithms has a significant impact on the final solution.

**Conclusion:** The PPO algorithm is better than the A2C algorithm in this case because it arrived at the most optimal solution.

The solution to this problem can be further improved by adjusting the gamma parameter or testing what happens with an additional increase in steps. Of course, we then need to consider whether it is worth waiting for a large number of training steps to complete for a specific improvement.

Finally, let's visualize the training process of all models through graphs...
"""

putanja_logova_modela_1 = projekat_folder + "logs_best_modelPPO[40,32]_100k_n_envs=10"
putanja_logova_modela_2 = projekat_folder + "logs_best_modelPPO[64,64]_100k_n_envs=10"
putanja_logova_modela_3 = projekat_folder + "logs_best_modelPPO[128,128]_100k_n_envs=10"
putanja_logova_modela_4 = projekat_folder + "logs_best_modelPPO[128,128]_1000k_n_envs=10"

logovi_modela_1 = np.load(putanja_logova_modela_1)
logovi_modela_2 = np.load(putanja_logova_modela_2)
logovi_modela_3 = np.load(putanja_logova_modela_3)
logovi_modela_4 = np.load(putanja_logova_modela_4)

x1 = logovi_modela_1['timesteps'][:]
y1 = np.mean(logovi_modela_1['results'][:], axis=1)
plt.plot(x1,y1,label=" prvi model",color="red")

x2 = logovi_modela_2['timesteps'][:]
y2 = np.mean(logovi_modela_2['results'][:], axis=1)
plt.plot(x2,y2,label=" drugi model",color="yellow")

x3 = logovi_modela_3['timesteps'][:]
y3 = np.mean(logovi_modela_3['results'][:], axis=1)
plt.plot(x3,y3,label=" treći model",color="green")

plt.legend(loc="lower right")

x4 = logovi_modela_4['timesteps'][:]
y4 = np.mean(logovi_modela_4['results'][:], axis=1)
plt.plot(x4,y4,label=" četvrti model",color="blue")

"""Here, only the best model is shown, trained on the largest number of steps compared to all other models.

# **Conclusion**

Here, we will compare the best models from each algorithm, graphically.
"""

x=['A2C','PPO','DQN']
y=[193,275.71,189.95]
e=[90.96,17.09,31.74]

plt.title("Poređenje algoritama")
plt.xlabel('algoritam')
plt.ylabel('nagrada')

plt.errorbar(x, y, e, linestyle='None', marker='o')

"""## **LunarLanderContinuous-v2**"""

import gym

ime_okruzenja = "LunarLanderContinuous-v2" # biramo okruženje za igranje.
okruzenje = gym.make(ime_okruzenja) # učitavamo odabrano okruženje, tj. problem.
spec = gym.spec(ime_okruzenja) # učitavamo specifikaciju odabranog okruženja
okruzenje.seed(42) # fiksiranje nasumicnosti da bi se okruzenje isto ponasalo kad ponovimo program

print(f"Akcije okruženja: { okruzenje.action_space }")
print(f"Dimenzije stanja: { okruzenje.observation_space }")
print(f"Maksimalan broj koraka: { spec.max_episode_steps }")
print(f"Opseg vrednosti nagrade: { okruzenje.reward_range }")
print(f"Prag nagrade: { spec.reward_threshold }")

"""# **DDQN**

This algorithm works exclusively with continuous data. It optimizes both the policy and the Q(s,a) value. Naturally, it also learns the state value function V(s). Therefore, it is quite complex.
"""

from stable_baselines3 import DDPG

"""First, we will start training with default values for 300k steps. We won't exaggerate with the number of steps because DDQN is much slower than other algorithms (complexity!), such as A2C and PPO. They, however, could be trained in a vectorized environment, which significantly sped things up. That is not the case here. It takes more than 3 hours for 300k steps!

At the same time, we will try changing gamma to, for example, 0.95 (it yielded results with A2C) and experiment with the action_noise.

We haven't had the chance to encounter it in previous algorithms. Indeed, it promotes exploration (entropy) and is not present by default in DDQN.
"""

# default
najbolji_model_eksperiment1_putanja = projekat_folder + "DDPG_300k"
logovi_evaluacije_eksperiment1_putanja = projekat_folder + "log_DDPG_300k"

eval_callback = EvalCallback(okruzenje, best_model_save_path=najbolji_model_eksperiment1_putanja,
                             log_path=logovi_evaluacije_eksperiment1_putanja, eval_freq=1000,
                             deterministic=True, render=False)


model = DDPG('MlpPolicy',okruzenje, verbose=1)
model.learn(300000,callback=eval_callback,log_interval=5000)

# action_noise
from stable_baselines3.common.noise import NormalActionNoise
najbolji_model_eksperiment1_putanja = projekat_folder + "DDPG_300k_noise"
logovi_evaluacije_eksperiment1_putanja = projekat_folder + "log_DDPG_300k_noise"

eval_callback = EvalCallback(okruzenje, best_model_save_path=najbolji_model_eksperiment1_putanja,
                             log_path=logovi_evaluacije_eksperiment1_putanja, eval_freq=1000,
                             deterministic=True, render=False)

n_actions=okruzenje.action_space.shape[-1]
action_noise=NormalActionNoise(mean=np.zeros(n_actions),sigma=0.1*np.ones(n_actions))

# nećemo preterivati sa noise-om, imajući u vidu exploataciju
# srednja vrednost je nula, a std 0.1 (u nizu dužine broja akcija)

model = DDPG('MlpPolicy', okruzenje,action_noise=action_noise,verbose=1)
model.learn(300000,callback=eval_callback,log_interval=5000)

najbolji_model_eksperiment1_putanja = projekat_folder + "DDPG_300k_gama=0.95"
logovi_evaluacije_eksperiment1_putanja = projekat_folder + "log_DDPG_300k_gama=0.95"

eval_callback = EvalCallback(okruzenje, best_model_save_path=najbolji_model_eksperiment1_putanja,
                             log_path=logovi_evaluacije_eksperiment1_putanja, eval_freq=1000,
                             deterministic=True, render=False)


model = DDPG('MlpPolicy',okruzenje,gamma=0.95, verbose=1)
model.learn(300000,callback=eval_callback,log_interval=5000)

"""**Training Progress**"""

# default
putanja_logova=projekat_folder+"log_DDPG_300k/evaluations.npz"
logovi=np.load(putanja_logova)

x = logovi['timesteps'][5:]
y = np.mean(logovi['results'][5:], axis=1)
# ne crtamo prvih 5 tačaka, jer su vrednosti na y - osi ogromne
# i ne vide se lepo nagrade na većem rednom broju koraka

f = plt.figure()
f.set_figwidth(12)
f.set_figheight(5)

plt.xlabel('korak')
plt.ylabel('nagrada')

plt.plot(x,y)

# action_noise
putanja_logova=projekat_folder+"log_DDPG_300k_noise/evaluations.npz"
logovi=np.load(putanja_logova)
x = logovi['timesteps'][5:]
y = np.mean(logovi['results'][5:], axis=1)


f = plt.figure()
f.set_figwidth(12)
f.set_figheight(5)

plt.xlabel('korak')
plt.ylabel('nagrada')

plt.plot(x,y)

"""Notice how much sharper the changes are here compared to the first graph - exploration."""

# gama=0.95
putanja_logova=projekat_folder+"log_DDPG_300k_gama=0.95/evaluations.npz"
logovi=np.load(putanja_logova)
x = logovi['timesteps'][5:]
y = np.mean(logovi['results'][5:], axis=1)


f = plt.figure()
f.set_figwidth(12)
f.set_figheight(5)

plt.xlabel('korak')
plt.ylabel('nagrada')

plt.plot(x,y)

"""From these graphs (especially the first and second), we can see that there is significant improvement at the last moment. One idea for creating a better model would then be to continue training for an additional, for example, 300k steps.

**Evaluation**
"""

# default
ime_okruzenja = "LunarLanderContinuous-v2"
okruzenje = gym.make(ime_okruzenja)
spec = gym.spec(ime_okruzenja)

model = DDPG.load(projekat_folder + "DDPG_300k/best_model")

mean_reward, std_reward = evaluate_policy(model, okruzenje, n_eval_episodes=100,deterministic=True)
print(f"Srednja nagrada: {mean_reward}, st. devijacija nagrade: {std_reward}")

#action_noise
ime_okruzenja = "LunarLanderContinuous-v2"
okruzenje = gym.make(ime_okruzenja)
spec = gym.spec(ime_okruzenja)

model = DDPG.load(projekat_folder + "DDPG_300k_noise/best_model")

mean_reward, std_reward = evaluate_policy(model, okruzenje, n_eval_episodes=100,deterministic=True)
print(f"Srednja nagrada: {mean_reward}, st. devijacija nagrade: {std_reward}")

#gama=0.95
from stable_baselines3.common.evaluation import evaluate_policy
ime_okruzenja = "LunarLanderContinuous-v2"
okruzenje = gym.make(ime_okruzenja)
spec = gym.spec(ime_okruzenja)

model = DDPG.load(projekat_folder + "DDPG_300k_gama=0.95/best_model")

mean_reward, std_reward = evaluate_policy(model, okruzenje, n_eval_episodes=100,deterministic=True)
print(f"Srednja nagrada: {mean_reward}, st. devijacija nagrade: {std_reward}")

"""**Conclusion**

The algorithm with default values yields the best results and would likely perform even better if the training is extended (e.g., for an additional 300k steps).
The same applies to the "action_noise model." Although it produces slightly worse results, the growth of its graph in the very last steps is the highest.

### **PPO - Continuous Problem**
"""

from stable_baselines3 import PPO

"""The **PPO** algorithm has the ability to be used on problems with discrete values as well as on problems with continuous values. Of course, these environments differ significantly, so the question arises: "*How will this change affect the performance of the PPO algorithm?*"
"""

ime_okruzenja = "LunarLanderContinuous-v2"
eval_okruzenje = gym.make(ime_okruzenja)
broj_okruzenja = 100
okruzenje = make_vec_env(ime_okruzenja, n_envs=broj_okruzenja)
okruzenje.seed(seed)

najbolji_model_putanja = projekat_folder + "best_model_[128,128]_1000k"
logovi_evaluacije_putanja = projekat_folder + "logs_best_model_[128,128]_1000k"

eval_callback = EvalCallback(eval_okruzenje, best_model_save_path=najbolji_model_putanja,
                             log_path=logovi_evaluacije_putanja, n_eval_episodes=100, eval_freq=1000,
                             deterministic=True, render=False)

policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=[dict(pi=[128,128], vf=[128,128])])
PPO_model = PPO("MlpPolicy", okruzenje, gamma = 0.99, policy_kwargs=policy_kwargs, verbose = 1)
PPO_model.learn(total_timesteps=1000000, log_interval=1000, callback=eval_callback)
PPO_model = PPO.load(najbolji_model_putanja + "/best_model")

from stable_baselines3.common.evaluation import evaluate_policy

eval_okruzenje.reset(seed=seed)
mean_reward, std_reward = evaluate_policy(PPO_model, eval_okruzenje, n_eval_episodes=100)
print(f"Srednja nagrada: {mean_reward}, st. devijacija nagrade: {std_reward}")

"""For the given parameters, the following results were obtained:

Mean reward: -47.05433856279508, reward standard deviation: 115.17427713034863

The model did not perform well with this configuration, so during the next training, we will use a different neural network.

Perhaps this one is too complex, so let's consider the initial neural networks for the policy and value function, which are automatically set by the PPO(...) method, with sizes [64, 64].
"""

najbolji_model_putanja = projekat_folder + "best_model_[64,64]_1000k"
logovi_evaluacije_putanja = projekat_folder + "logs_best_model_[64,64]_1000k"

eval_callback = EvalCallback(eval_okruzenje, best_model_save_path=najbolji_model_putanja,
                             log_path=logovi_evaluacije_putanja, n_eval_episodes=100, eval_freq=1000,
                             deterministic=True, render=False)

policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=[dict(pi=[64,64], vf=[64,64])])
PPO_model = PPO("MlpPolicy", okruzenje, gamma = 0.99, policy_kwargs=policy_kwargs, verbose = 1)
PPO_model.learn(total_timesteps=1000000, log_interval=1000, callback=eval_callback)
PPO_model = PPO.load(najbolji_model_putanja + "/best_model")

eval_okruzenje.reset(seed=seed)
mean_reward, std_reward = evaluate_policy(PPO_model, eval_okruzenje, n_eval_episodes=100)
print(f"Srednja nagrada: {mean_reward}, st. devijacija nagrade: {std_reward}")

"""This time, the model achieved the following results:
Mean reward: -83.21557353773886, reward standard deviation: 99.29833248278982.

The conclusion is that the problem is not simple enough for this neural network to solve optimally. Therefore, we will now significantly increase the size of the network to [200, 200].
"""

najbolji_model_putanja = projekat_folder + "best_model_[200,200]_1000k"
logovi_evaluacije_putanja = projekat_folder + "logs_best_model_[200,200]_1000k"

eval_callback = EvalCallback(eval_okruzenje, best_model_save_path=najbolji_model_putanja,
                             log_path=logovi_evaluacije_putanja, n_eval_episodes=100, eval_freq=1000,
                             deterministic=True, render=False)

policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=[dict(pi=[200,200], vf=[200,200])])
PPO_model = PPO("MlpPolicy", okruzenje, gamma = 0.99, policy_kwargs=policy_kwargs, verbose = 1)
PPO_model.learn(total_timesteps=1000000, log_interval=1000, callback=eval_callback)
PPO_model = PPO.load(najbolji_model_putanja + "/best_model")

eval_okruzenje.reset(seed=seed)
mean_reward, std_reward = evaluate_policy(PPO_model, eval_okruzenje, n_eval_episodes=100)
print(f"Srednja nagrada: {mean_reward}, st. devijacija nagrade: {std_reward}")

"""Although it would be expected for the results to improve, it is evident that this is not the case, and now we must choose the next change to make.  

Mean reward: -85.32770930034873, reward standard deviation: 77.65885717087552  

If we assume that more is always better, the only option left is to adjust gamma. Let's start with a value of 0.85.
"""

najbolji_model_putanja = projekat_folder + "best_model_[200,200]_gamma=0.85_1000k"
logovi_evaluacije_putanja = projekat_folder + "logs_best_model_[200,200]_gamma=0.85_1000k"

eval_callback = EvalCallback(eval_okruzenje, best_model_save_path=najbolji_model_putanja,
                             log_path=logovi_evaluacije_putanja, n_eval_episodes=100, eval_freq=1000,
                             deterministic=True, render=False)

policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=[dict(pi=[200,200], vf=[200,200])])
PPO_model = PPO("MlpPolicy", okruzenje, gamma = 0.85, policy_kwargs=policy_kwargs, verbose = 1)
PPO_model.learn(total_timesteps=1000000, log_interval=1000, callback=eval_callback)
PPO_model = PPO.load(najbolji_model_putanja + "/best_model")

eval_okruzenje.reset(seed=seed)
mean_reward, std_reward = evaluate_policy(PPO_model, eval_okruzenje, n_eval_episodes=100)
print(f"Srednja nagrada: {mean_reward}, st. devijacija nagrade: {std_reward}")

"""The results are, contrary to expectations, even worse:  

Mean reward: -117.57555621724569, reward standard deviation: 108.5625385511828  

This suggests that the real issue lies in the neural network—it has now become too complex, and we need to adjust it to a value between 128 and 200. Let's start with values closer to 128, as we previously obtained better results at that point, and set the size to 140!
"""

najbolji_model_putanja = projekat_folder + "best_model_[140,140]_1000k"
logovi_evaluacije_putanja = projekat_folder + "logs_best_model_[140,140]_1000k"

eval_callback = EvalCallback(eval_okruzenje, best_model_save_path=najbolji_model_putanja,
                             log_path=logovi_evaluacije_putanja, n_eval_episodes=100, eval_freq=1000,
                             deterministic=True, render=False)

policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=[dict(pi=[140,140], vf=[140,140])])
PPO_model = PPO("MlpPolicy", okruzenje, gamma = 0.99, policy_kwargs=policy_kwargs, verbose = 1)
PPO_model.learn(total_timesteps=1000000, log_interval=1000, callback=eval_callback)
PPO_model = PPO.load(najbolji_model_putanja + "/best_model")

eval_okruzenje.reset(seed=seed)
mean_reward, std_reward = evaluate_policy(PPO_model, eval_okruzenje, n_eval_episodes=100)
print(f"Srednja nagrada: {mean_reward}, st. devijacija nagrade: {std_reward}")

"""The results were not encouraging...  

Mean reward: -73.08620833078257, reward standard deviation: 106.09698621991414  

And when we changed gamma to 0.90, the results worsened slightly!  

Mean reward: -93.97449970734544, reward standard deviation: 130.38959741403835

However, the problem lies, as in a typical case, in the shape of the learning curve.
"""

putanja_logova = logovi_evaluacije_putanja + "/evaluations.npz"
logovi_eksperimenata = np.load(putanja_logova)

import matplotlib.pyplot as plt

x = logovi_eksperimenata['timesteps'][:]
y = np.mean(logovi_eksperimenata['results'][:], axis=1)
plt.xlabel="koraci"
plt.ylabel="rezultati"
plt.plot(x,y)

print(x)
print(y)

"""Although at first glance it seems like the graph won't surpass the line above it, it appears to continue growing. This indicates that more steps are needed for the model to train properly. So, let's increase the number of steps to 1.5 million."""

najbolji_model_putanja = projekat_folder + "best_model_[140,140]_1500k"
logovi_evaluacije_putanja = projekat_folder + "logs_best_model_[140,140]_1500k"

eval_callback = EvalCallback(eval_okruzenje, best_model_save_path=najbolji_model_putanja,
                             log_path=logovi_evaluacije_putanja, n_eval_episodes=100, eval_freq=1000,
                             deterministic=True, render=False)

policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=[dict(pi=[140,140], vf=[140,140])])
PPO_model = PPO("MlpPolicy", okruzenje, gamma = 0.99, policy_kwargs=policy_kwargs, verbose = 1)
PPO_model.learn(total_timesteps=1500000, log_interval=1000, callback=eval_callback)
PPO_model = PPO.load(najbolji_model_putanja + "/best_model")

eval_okruzenje.reset(seed=seed)
mean_reward, std_reward = evaluate_policy(PPO_model, eval_okruzenje, n_eval_episodes=100)
print(f"Srednja nagrada: {mean_reward}, st. devijacija nagrade: {std_reward}")

putanja_logova = logovi_evaluacije_putanja + "/evaluations.npz"
logovi_eksperimenata = np.load(putanja_logova)

x = logovi_eksperimenata['timesteps'][:]
y = np.mean(logovi_eksperimenata['results'][:], axis=1)
plt.xlabel="koraci"
plt.ylabel="rezultati"
plt.plot(x,y)

print(x)
print(y)

"""After this change, the results improved:  

Mean reward: 168.2776823527179, reward standard deviation: 55.198697533521376  

However, the graph still suggests that the model could benefit from more steps. Let's try increasing it to 2 million.
"""

najbolji_model_putanja = projekat_folder + "best_model_[140,140]_2000k"
logovi_evaluacije_putanja = projekat_folder + "logs_best_model_[140,140]_2000k"

eval_callback = EvalCallback(eval_okruzenje, best_model_save_path=najbolji_model_putanja,
                             log_path=logovi_evaluacije_putanja, n_eval_episodes=100, eval_freq=1000,
                             deterministic=True, render=False)

policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=[dict(pi=[140,140], vf=[140,140])])
PPO_model = PPO("MlpPolicy", okruzenje, gamma = 0.99, policy_kwargs=policy_kwargs, verbose = 1)
PPO_model.learn(total_timesteps=2000000, log_interval=1000, callback=eval_callback)
PPO_model = PPO.load(najbolji_model_putanja + "/best_model")

eval_okruzenje.reset(seed=seed)
mean_reward, std_reward = evaluate_policy(PPO_model, eval_okruzenje, n_eval_episodes=100)
print(f"Srednja nagrada: {mean_reward}, st. devijacija nagrade: {std_reward}")

putanja_logova = logovi_evaluacije_putanja + "/evaluations.npz"
logovi_eksperimenata = np.load(putanja_logova)

x = logovi_eksperimenata['timesteps'][:]
y = np.mean(logovi_eksperimenata['results'][:], axis=1)
plt.xlabel="koraci"
plt.ylabel="rezultati"
plt.plot(x,y)

print(x)
print(y)

"""The reward has increased to:  

Mean reward: 226.5595655714492, reward standard deviation: 61.51975696960877  

This means that our model often surpasses the reward threshold, indicating that we have successfully addressed that issue to some extent.

**Conclusion:**  

A few important points should be noted here. First, although the [128, 128] network performed better for this problem compared to [140, 140], the improvement observed on both networks was primarily due to the error being linked to the number of steps.  

Of course, this problem can be further refined by selecting the [128, 128] network and then finding the optimal gamma value.  

Finally, the environment itself proved to be highly demanding for the algorithm, as it required significantly more steps for the agent to "adapt" to this complex setting with unusual rules.
"""